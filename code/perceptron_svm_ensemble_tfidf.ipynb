{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from word2number import w2n\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split_data(filename):\n",
    "    file = open(filename, \"r\").read()\n",
    "    list_rows = []\n",
    "    file = file.split(\"\\n\")\n",
    "    for row in range(0, len(file) - 1):\n",
    "        dict_rows = {}\n",
    "        splits = file[row].split(\" \")\n",
    "        for s in range(len(splits)):\n",
    "            if s == 0:\n",
    "                if int(splits[s]) == 1:\n",
    "                    dict_rows[s] =  int(splits[0])\n",
    "                else:\n",
    "                    dict_rows[s] = -1\n",
    "            else:\n",
    "                index,val = [float(e) for e in splits[s].split(':')]\n",
    "                dict_rows[index] = val\n",
    "        list_rows.append(dict_rows)\n",
    "    df =  pd.DataFrame.from_dict(list_rows)\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_perceptron(data,hp,ep):\n",
    "    # initialize weights and bias term randomly between -0.01 and 0.01\n",
    "    np.random.seed(7)\n",
    "    update = 0\n",
    "    av_w = np.zeros(shape=data.shape[1]-1)\n",
    "    av_b = 0\n",
    "    w = np.random.uniform(-0.01, 0.01, size=data.shape[1] - 1)\n",
    "    b = np.random.uniform(-0.01, 0.01)\n",
    "    dict_epoch_acc = {}\n",
    "    learning_rate = hp\n",
    "    for i in range(ep):\n",
    "        accuracy = 0\n",
    "        np.random.shuffle(data)\n",
    "        for r in range(len(data)):\n",
    "            ground_truth = data[r, 0]\n",
    "            sample = data[r, 1:]\n",
    "            if np.dot(np.transpose(w), sample) + b <= 0:\n",
    "                prediction = -1\n",
    "            else:\n",
    "                prediction = 1\n",
    "            if int(ground_truth) != int(prediction):\n",
    "                update += 1\n",
    "                w = w + learning_rate * ground_truth * sample\n",
    "                b = b + learning_rate * ground_truth\n",
    "            else:\n",
    "                accuracy += 1\n",
    "            av_w = av_w + w\n",
    "            av_b = av_b + b\n",
    "        dict_epoch_acc[i] = av_w, av_b, (accuracy / len(data))\n",
    "    return  dict_epoch_acc,update\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(we, bi, test_data):\n",
    "    accuracy = 0\n",
    "    for r in range(len(test_data)):\n",
    "        ground_truth = test_data[r,0]\n",
    "        sample = list(test_data[r,:])\n",
    "        sample.pop(0)\n",
    "        prediction = -1 if np.dot(np.transpose(we), sample) + bi <= 0 else 1\n",
    "        if prediction == ground_truth:\n",
    "            accuracy = accuracy + 1\n",
    "    return (accuracy / len(test_data)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max(dict):\n",
    "    acc = 0\n",
    "    we_training = []\n",
    "    bias_training = 0\n",
    "    for key, value in dict.items():\n",
    "        if acc < value[2]:\n",
    "            acc = value[2]\n",
    "            we_training = value[0]\n",
    "            bias_training = value[1]\n",
    "    return we_training,bias_training,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(f1,f2,f3,f4,f5):\n",
    "\n",
    "    best_h = 0\n",
    "    max_acc = 0\n",
    "    hyper_paramter = [0.1,1,0.01]\n",
    "    for h in hyper_paramter:\n",
    "        acc = 0\n",
    "        #run for f1 as test:\n",
    "        frames = [f2, f3, f4, f5]\n",
    "        train =np.concatenate((f2,f3,f4,f5),axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1,b1,a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1,b1,f1)\n",
    "        #run for f2 as test\n",
    "        frames = [f1, f3, f4, f5]\n",
    "        train = np.concatenate((f1,f3,f4,f5),axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f2)\n",
    "        #run for f3 as test\n",
    "        frames = [f2, f1, f4, f5]\n",
    "        train = np.concatenate((f1, f2, f4, f5), axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f3)\n",
    "        #run for f4 as test\n",
    "        frames = [f2, f1, f3, f5]\n",
    "        train = np.concatenate((f1, f3, f2, f5), axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f4)\n",
    "        #run for f5 as test\n",
    "        frames = [f2, f1, f4, f3]\n",
    "        train = np.concatenate((f1, f3, f4, f2), axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f5)\n",
    "        if max_acc < acc/5:\n",
    "            max_acc = acc/5\n",
    "            best_h = h\n",
    "    return best_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_split_data('tfidf.train.libsvm')\n",
    "df_test = read_split_data('tfidf.test.libsvm')\n",
    "df_eval = read_split_data('tfidf.eval.anon.libsvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_test = list(df_test.columns)\n",
    "for i in range(0,10001):\n",
    "    if i not in cols_test:\n",
    "        df_test[i] = [0.0 for i in range(len(df_test))]\n",
    "\n",
    "\n",
    "cols_eval = list(df_eval.columns)\n",
    "for i in range(0,10001):\n",
    "    if i not in cols_eval:\n",
    "        df_eval[i] = [0.0 for i in range(len(df_eval))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.reindex(sorted(df_test.columns), axis = 1)\n",
    "df_train = df_train.reindex(sorted(df_train.columns), axis = 1)\n",
    "df_eval =  df_eval.reindex(sorted(df_eval.columns),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numpy = df_train.to_numpy()\n",
    "test_numpy = df_test.to_numpy()\n",
    "eval_numpy =  df_eval.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = train_numpy[5000:7500,:]\n",
    "fold2 = train_numpy[7500:10000,:]\n",
    "fold3 = train_numpy[10000:12500,:]\n",
    "fold4 = train_numpy[12500:15000,:]\n",
    "fold5 = train_numpy[15000:17500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = crossvalidation(fold1,fold2,fold3,fold4,fold5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= {}\n",
    "bias = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = np.copy(train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    np.random.shuffle(train1)\n",
    "    sample = train1[:1750,:]    \n",
    "    dict_training_per,u = batch_perceptron(sample, best_lr, 10)\n",
    "    w,b,a = cal_max(dict_training_per)\n",
    "    weights[i] = w\n",
    "    bias[i] = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(we,b,sample):\n",
    "#     sample.pop(0)\n",
    "    prediction = -1 if np.dot(np.transpose(we), sample) + b <= 0 else 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataset(weights,bias,data):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        row = []\n",
    "        row.append(data[i,0])\n",
    "        for key,value in weights.items():\n",
    "            x = data[i,1:]\n",
    "#             del prediction_rows[:]\n",
    "            p=prediction(value,bias[key],x)\n",
    "#             p = prediction_rows[0]\n",
    "            row.append(p)\n",
    "        l.append(row)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataset_eval(weights,bias,data):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        row = []\n",
    "#         row.append(data[i,0])\n",
    "        for key,value in weights.items():\n",
    "            x = data[i,:]\n",
    "#             del prediction_rows[:]\n",
    "            p=prediction(value,bias[key],x)\n",
    "#             p = prediction_rows[0]\n",
    "            row.append(p)\n",
    "        l.append(row)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_svm = new_dataset(weights,bias,train_numpy)\n",
    "test_svm = new_dataset(weights,bias,test_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17500, 10001)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_svm = new_dataset(weights,bias,eval_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(data,lr,C,ep):\n",
    "    #initialize weights and bias term\n",
    "\n",
    "    lr_0 = lr\n",
    "    loss = 0.0\n",
    "    dict_epoch_accuracy = {}\n",
    "    dict_epoch_loss = {}\n",
    "    prev_loss = float(\"inf\")\n",
    "    seed(7)\n",
    "    # print(data.shape[1])\n",
    "    w = np.random.uniform(-0.01,0.01,size=data.shape[1])\n",
    "    # for i in range(len(data.columns)):\n",
    "    #     w.append(uniform(-0.01,0.01))\n",
    "    # print(len(w))\n",
    "    #for epoch as per threshold\n",
    "    for i in range(ep):\n",
    "        # print(\"for epoch\",i)\n",
    "\n",
    "        lr = lr/(1+i)\n",
    "        np.random.shuffle(data)\n",
    "        # data =  data.sample(frac=1,random_state=1)\n",
    "        for row in range(len(data)):\n",
    "            y = data[row,0]\n",
    "            x = data[row,1:]\n",
    "            # x.pop(0)\n",
    "            x = np.append(x,1)\n",
    "\n",
    "            # print(\"len of x\",len(x))\n",
    "            if np.dot((np.transpose(w)),x)*y <= 1:\n",
    "                w = ((1-lr)*w) + ((lr*C*y)*x)\n",
    "            else:\n",
    "                w = (1-lr)*w\n",
    "\n",
    "        dict_epoch_accuracy[i] = i,evaluate(w,data),w\n",
    "        # print(\"accuracy\",accuracy)\n",
    "            ## calculate the loss/value of the objective function\n",
    "        # loss = (1/2)*np.dot(np.transpose(w),w)\n",
    "        # for row in range(len(data)):\n",
    "        #     y1 = data[row,0]\n",
    "        #     x1 = data[row,1:]\n",
    "        #     x1 = np.append(x1, 1)\n",
    "        #     # loss = round(loss + max(0,(1- (y1 * np.dot(np.transpose(w),x1)))),4)\n",
    "        #     loss = loss + (C*max(0, (1 - (y1 * np.dot(np.transpose(w), x1)))))\n",
    "        loss = (1/2)*np.dot(np.transpose(w),w)\n",
    "        l = 0\n",
    "        # loss = (1/2)*np.dot(np.transpose(w),w)\n",
    "        for row in range(len(data)):\n",
    "            y1 = data[row,0]\n",
    "            x1 = data[row,1:]\n",
    "            x1 = np.append(x1, 1)\n",
    "            # loss = round(loss + max(0,(1- (y1 * np.dot(np.transpose(w),x1)))),4)\n",
    "            l =  l + (max(0, (1 - (y1 * np.dot(np.transpose(w), x1)))))\n",
    "        loss = loss + C*l\n",
    "        # print(\"loss\",loss)\n",
    "        dict_epoch_loss[i] = loss\n",
    "        if abs(prev_loss - loss)  < 0.005:\n",
    "            # print(\"threshold epoch\",i)\n",
    "            break\n",
    "        prev_loss = loss\n",
    "\n",
    "    return dict_epoch_accuracy,dict_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_cross_validation(data,lr,C,ep):\n",
    "    #initialize weights and bias term\n",
    "    w = []\n",
    "    lr_0 = lr\n",
    "    loss = 0.0\n",
    "    dict_epoch_accuracy = {}\n",
    "    dict_epoch_loss = {}\n",
    "    prev_loss =  float(\"inf\")\n",
    "\n",
    "    # print(data.shape[1])\n",
    "    w = np.random.uniform(-0.01,0.01,size=data.shape[1])\n",
    "    # for i in range(len(data.columns)):\n",
    "    #     w.append(uniform(-0.01,0.01))\n",
    "    # print(len(w))\n",
    "    #for epoch as per threshold\n",
    "    for i in range(ep):\n",
    "        # print(\"for epoch\",i)\n",
    "        accuracy = 0\n",
    "        lr = lr_0/(1+i)\n",
    "        np.random.shuffle(data)\n",
    "        # data =  data.sample(frac=1,random_state=1)\n",
    "        for row in range(len(data)):\n",
    "            y = data[row,0]\n",
    "            x = data[row,1:]\n",
    "            # x.pop(0)\n",
    "            x = np.append(x,1)\n",
    "\n",
    "            # print(\"len of x\",len(x))\n",
    "            if np.dot((np.transpose(w)),x)*y <= 1:\n",
    "                w = ((1-lr)*w) + ((lr*C*y)*x)\n",
    "            else:\n",
    "                w = (1-lr)*w\n",
    "                accuracy = accuracy + 1\n",
    "        dict_epoch_accuracy[i] = i,evaluate(w,data),w\n",
    "    return dict_epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max(d):\n",
    "    max_w = []\n",
    "    max_accuracy = -1\n",
    "    epoch = 0\n",
    "    for key, value in d.items():\n",
    "        if value[1] > max_accuracy:\n",
    "            max_accuracy = value[1]\n",
    "            max_w = value[2]\n",
    "            epoch = value[0]\n",
    "        # if max_accuracy == 0:\n",
    "    # print(len(max_w))\n",
    "    return  epoch, max_accuracy,max_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(we, test_data):\n",
    "    accuracy = 0\n",
    "    for r in range(len(test_data)):\n",
    "        y = test_data[r,0]\n",
    "        x = test_data[r,1:]\n",
    "        # sample.pop(0)\n",
    "        x = np.append(x,1)\n",
    "        # print(len(we))\n",
    "        # print(len(x))\n",
    "        prediction = -1 if np.dot(np.transpose(we), x) <= 0 else 1\n",
    "        # if np.dot((np.transpose(we)),x)*y >= 1:\n",
    "        if prediction == y:\n",
    "            accuracy = accuracy + 1\n",
    "    return (accuracy / len(test_data)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(f1,f2,f3,f4,f5):\n",
    "    lr = [10**0,10**-1,10**-2,10**-3,10**-4,10**-5]\n",
    "    C = [10**3,10**2,10**1,10**0,10**-1,10**-2]\n",
    "    best_lr=0\n",
    "    best_c = 0\n",
    "    max_acc = 0\n",
    "    for c in C:\n",
    "        for l in lr:\n",
    "            # print(\"running for learning rate:\",l,\"C:\",c)\n",
    "            acc = 0\n",
    "            # run for f1 as test:\n",
    "            # print(\"f1 as test\")\n",
    "            frames = [f2, f3, f4, f5]\n",
    "            train = np.concatenate((f2,f3,f4,f5),axis=0)\n",
    "            # print(\"len\", len(f2.columns))\n",
    "            # print(\"len\", len(f3.columns))\n",
    "            # print(\"len\", len(f4.columns))\n",
    "            # print(\"len\", len(f5.columns))\n",
    "            # train = pd.concat(frames)\n",
    "            # print(\"len\",len(train.columns))\n",
    "            # dict_acc_w= svm_cross_validation(train, l, c, 20)\n",
    "            dict_acc_w = svm_cross_validation(train, l, c, 20)\n",
    "            e,a,w = cal_max(dict_acc_w)\n",
    "            acc = acc + evaluate(w, f1)\n",
    "            # run for f2 as test\n",
    "            # print(\"f2 as test\")\n",
    "            frames = [f1, f3, f4, f5]\n",
    "            train = np.concatenate((f1,f3,f4,f5),axis=0)\n",
    "            # train = pd.concat(frames)\n",
    "            dict_acc_w= svm_cross_validation(train, l, c, 20)\n",
    "            e,a,w = cal_max(dict_acc_w)\n",
    "            acc = acc + evaluate(w ,f2)\n",
    "            # run for f3 as test\n",
    "            # print(\"f3 as test\")\n",
    "            # frames = [f2, f1, f4, f5]\n",
    "            # train = pd.concat(frames)\n",
    "            train = np.concatenate((f1, f2, f4, f5), axis=0)\n",
    "            dict_acc_w= svm_cross_validation(train, l, c, 20)\n",
    "            e,a,w = cal_max(dict_acc_w)\n",
    "            acc = acc + evaluate(w, f3)\n",
    "            # run for f4 as test\n",
    "            # print(\"f4 as test\")\n",
    "            # frames = [f2, f1, f3, f5]\n",
    "            # train = pd.concat(frames)\n",
    "            train = np.concatenate((f1, f3, f2, f5), axis=0)\n",
    "            dict_acc_w= svm_cross_validation(train, l, c, 20)\n",
    "            e,a,w = cal_max(dict_acc_w)\n",
    "            acc = acc + evaluate(w, f4)\n",
    "            # run for f5 as test\n",
    "            # print(\"f5 as test\")\n",
    "            # frames = [f2, f1, f4, f3]\n",
    "            # train = pd.concat(frames)\n",
    "            train = np.concatenate((f1, f3, f4, f2), axis=0)\n",
    "            dict_acc_w= svm_cross_validation(train, l, c, 20)\n",
    "            e,a,w = cal_max(dict_acc_w)\n",
    "            acc = acc + evaluate(w, f5)\n",
    "            acc = acc/5\n",
    "            if max_acc<acc:\n",
    "                max_acc = acc\n",
    "                best_c =c\n",
    "                best_lr = l\n",
    "    print(\"best parameters\", \"accuracy\",max_acc,\"C:\",best_c,\"lr:\",best_lr)\n",
    "            # dict_margin_lr[(m, lr)] = acc / 5\n",
    "            # print(dict_margin_lr.keys())\n",
    "    return max_acc,best_c,best_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fold1_svm = new_dataset(weights,bias,fold1)    \n",
    "    fold2_svm = new_dataset(weights,bias, fold2)\n",
    "    fold3_svm = new_dataset(weights,bias, fold3)\n",
    "    fold4_svm = new_dataset(weights,bias, fold4)\n",
    "    fold5_svm = new_dataset(weights,bias, fold5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters accuracy 78.096 C: 0.1 lr: 0.01\n"
     ]
    }
   ],
   "source": [
    "a,c,lr = crossvalidation(fold1_svm,fold2_svm,fold3_svm,fold4_svm,fold5_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_a_w, dict_loss = svm(train_svm, lr, c, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, a, w = cal_max(dict_a_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.85142857142857"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(w, train_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.35555555555555"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(w, test_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_e(we, test_data):\n",
    "    accuracy = 0\n",
    "    p = []\n",
    "    for r in range(len(test_data)):\n",
    "        \n",
    "        x = test_data[r,1:]\n",
    "        \n",
    "        x = np.append(x,1)\n",
    "        # print(len(we))\n",
    "        # print(len(x))\n",
    "        prediction = -1 if np.dot(np.transpose(we), x) <= 0 else 1\n",
    "        p.append(prediction)\n",
    "        # if np.dot((np.transpose(we)),x)*y >= 1:\n",
    "#         if prediction == y:\n",
    "#             accuracy = accuracy + 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "p= evaluate_e(w, eval_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perceptron_svm_ensemble_tfidf.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"example_id\", \"label\"])\n",
    "    for i in range(len(p)):\n",
    "        if p[i]== -1:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = 1\n",
    "        writer.writerow([i, val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
