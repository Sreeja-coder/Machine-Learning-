{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from word2number import w2n\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split_data(filename):\n",
    "    file = open(filename, \"r\").read()\n",
    "    list_rows = []\n",
    "    file = file.split(\"\\n\")\n",
    "    for row in range(0, len(file) - 1):\n",
    "        dict_rows = {}\n",
    "        splits = file[row].split(\" \")\n",
    "        for s in range(len(splits)):\n",
    "            if s == 0:\n",
    "                if int(splits[s]) == 1:\n",
    "                    dict_rows[s] =  int(splits[0])\n",
    "                else:\n",
    "                    dict_rows[s] = -1\n",
    "            else:\n",
    "                index,val = [float(e) for e in splits[s].split(':')]\n",
    "                dict_rows[index] = val\n",
    "        list_rows.append(dict_rows)\n",
    "    df =  pd.DataFrame.from_dict(list_rows)\n",
    "    df = df.fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glove_train = read_split_data('glove.train.libsvm')\n",
    "df_glove_test = read_split_data('glove.test.libsvm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  pd.read_csv('misc-attributes-train.csv')\n",
    "eval =  pd.read_csv('misc-attributes-eval.csv')\n",
    "test = pd.read_csv('misc-attributes-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "defendants_age = []\n",
    "defendant_gender=[]\n",
    "num_victims=[]\n",
    "offence_category=[]\n",
    "offence_subcategory=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_age(x):\n",
    "    \n",
    "    for i,age in enumerate(x['defendant_age']):\n",
    "#         print(\"age\",age)\n",
    "        # print(i)\n",
    "        age = age.strip(\"(  ) -\")\n",
    "        if age != \"not known\":\n",
    "            age = age.replace(\"years\",\"\")\n",
    "            age = age.replace(\"about\", \"\")\n",
    "            age = age.replace(\"age\",\"\")\n",
    "            age = age.replace(\"of\", \"\" )\n",
    "            age = age.replace(\"old\", \"\")\n",
    "            age = age.strip()\n",
    "            if age.find(\" \") >= 0:\n",
    "                temp = age.split(\" \")\n",
    "                # print(temp)\n",
    "                age = '-'.join(temp)\n",
    "            syns = wordnet.synsets(age.strip())\n",
    "            # print(\"A\",age)\n",
    "            # print(\"S\",syns[0].lemmas()[0].name())\n",
    "            age = syns[0].lemmas()[0].name()\n",
    "            if age.find(\"-\") >= 0:\n",
    "                temp = age.split(\"-\")\n",
    "                # print(temp)\n",
    "                age = ' '.join(temp)\n",
    "            # print(age.strip())\n",
    "            defendants_age.append(w2n.word_to_num(age.strip()))\n",
    "        else:\n",
    "            defendants_age.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_age(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = ['female', 'indeterminate', 'male']\n",
    "off_cat = ['breakingPeace','damage','deception','kill','miscellaneous','royalOffences','sexual','theft','violentTheft']\n",
    "off_sub_cat = ['animalTheft','arson','assault','assaultWithIntent','assaultWithSodomiticalIntent','bankrupcy','bigamy','burglary','coiningOffences',\n",
    " 'concealingABirth', 'conspiracy', 'embezzlement','extortion','forgery','fraud','gameLawOffence','grandLarceny','highwayRobbery',\n",
    " 'housebreaking','illegalAbortion','indecentAssault','infanticide','keepingABrothel','kidnapping','libel','mail','manslaughter',\n",
    " 'murder','other','perjury','pervertingJustice','pettyLarceny','pettyTreason','piracy','pocketpicking','rape','receiving',\n",
    " 'religiousOffences','returnFromTransportation','riot','robbery','seditiousLibel','seditiousWords','seducingAllegiance',\n",
    " 'shoplifting','simpleLarceny','sodomy','stealingFromMaster','taxOffences','theftFromPlace','threateningBehaviour','treason',\n",
    " 'wounding','coiningOffences','vagabond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_gender(x):\n",
    "    for i,g in enumerate(x['defendant_gender']):\n",
    "        if g in gender:\n",
    "            defendant_gender.append(gender.index(g))\n",
    "#     return defendant_gender\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_offence_category(x):\n",
    "    for i,off in enumerate(x['offence_category']):\n",
    "        if off in off_cat:\n",
    "            offence_category.append(off_cat.index(off))\n",
    "#     return offence_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_offence_subcategory(x):\n",
    "    for i,off_sub in enumerate(x['offence_subcategory']):\n",
    "        if off_sub in off_sub_cat:\n",
    "            offence_subcategory.append(off_sub_cat.index(off_sub))\n",
    "#     return offence_subcategory\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_victims = list(dataset['num_victims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df_glove_train.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorize_gender(dataset)\n",
    "factorize_offence_category(dataset)\n",
    "factorize_offence_subcategory(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(labels,defendants_age,defendant_gender,num_victims,offence_category,offence_subcategory)),  columns =[\"label\",\"defendants_age\" ,\"defendants_gender\",\"num_victims\",\"offence_category\",\"offence_sub_category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numpy = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_perceptron(data,hp,ep):\n",
    "    # initialize weights and bias term randomly between -0.01 and 0.01\n",
    "    np.random.seed(7)\n",
    "    update = 0\n",
    "    av_w = np.zeros(shape=data.shape[1]-1)\n",
    "    av_b = 0\n",
    "    w = np.random.uniform(-0.01, 0.01, size=data.shape[1] - 1)\n",
    "    b = np.random.uniform(-0.01, 0.01)\n",
    "    dict_epoch_acc = {}\n",
    "    learning_rate = hp\n",
    "    for i in range(ep):\n",
    "        accuracy = 0\n",
    "        np.random.shuffle(data)\n",
    "        for r in range(len(data)):\n",
    "            ground_truth = data[r, 0]\n",
    "            sample = data[r, 1:]\n",
    "            if np.dot(np.transpose(w), sample) + b <= 0:\n",
    "                prediction = -1\n",
    "            else:\n",
    "                prediction = 1\n",
    "            if int(ground_truth) != int(prediction):\n",
    "                update += 1\n",
    "                w = w + learning_rate * ground_truth * sample\n",
    "                b = b + learning_rate * ground_truth\n",
    "            else:\n",
    "                accuracy += 1\n",
    "            av_w = av_w + w\n",
    "            av_b = av_b + b\n",
    "        dict_epoch_acc[i] = av_w, av_b, (accuracy / len(data))\n",
    "    return  dict_epoch_acc,update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(we, bi, test_data):\n",
    "    accuracy = 0\n",
    "    for r in range(len(test_data)):\n",
    "        ground_truth = test_data[r,0]\n",
    "        sample = list(test_data[r,:])\n",
    "        sample.pop(0)\n",
    "        prediction = -1 if np.dot(np.transpose(we), sample) + bi <= 0 else 1\n",
    "        if prediction == ground_truth:\n",
    "            accuracy = accuracy + 1\n",
    "    return (accuracy / len(test_data)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_max(dict):\n",
    "    acc = 0\n",
    "    we_training = []\n",
    "    bias_training = 0\n",
    "    for key, value in dict.items():\n",
    "        if acc < value[2]:\n",
    "            acc = value[2]\n",
    "            we_training = value[0]\n",
    "            bias_training = value[1]\n",
    "    return we_training,bias_training,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(f1,f2,f3,f4,f5):\n",
    "\n",
    "    best_h = 0\n",
    "    max_acc = 0\n",
    "    hyper_paramter = [0.1,1,0.01]\n",
    "    for h in hyper_paramter:\n",
    "        acc = 0\n",
    "        #run for f1 as test:\n",
    "        frames = [f2, f3, f4, f5]\n",
    "        train =np.concatenate((f2,f3,f4,f5),axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1,b1,a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1,b1,f1)\n",
    "        #run for f2 as test\n",
    "        frames = [f1, f3, f4, f5]\n",
    "        train = np.concatenate((f1,f3,f4,f5),axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f2)\n",
    "        #run for f3 as test\n",
    "        frames = [f2, f1, f4, f5]\n",
    "        train = np.concatenate((f1, f2, f4, f5), axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f3)\n",
    "        #run for f4 as test\n",
    "        frames = [f2, f1, f3, f5]\n",
    "        train = np.concatenate((f1, f3, f2, f5), axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f4)\n",
    "        #run for f5 as test\n",
    "        frames = [f2, f1, f4, f3]\n",
    "        train = np.concatenate((f1, f3, f4, f2), axis=0)\n",
    "        d, u = batch_perceptron(train, h, 10)\n",
    "        w1, b1, a1 = cal_max(d)\n",
    "        acc = acc + evaluate(w1, b1, f5)\n",
    "        if max_acc < acc/5:\n",
    "            max_acc = acc/5\n",
    "            best_h = h\n",
    "    return best_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold1 = train_numpy[5000:7500,:]\n",
    "fold2 = train_numpy[7500:10000,:]\n",
    "fold3 = train_numpy[10000:12500,:]\n",
    "fold4 = train_numpy[12500:15000,:]\n",
    "fold5 = train_numpy[15000:17500,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr = crossvalidation(fold1,fold2,fold3,fold4,fold5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= {}\n",
    "bias = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = np.copy(train_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    np.random.shuffle(train1)\n",
    "    sample = train1[:1750,:]    \n",
    "    dict_training_per,u = batch_perceptron(sample, best_lr, 10)\n",
    "    w,b,a = cal_max(dict_training_per)\n",
    "    weights[i] = w\n",
    "    bias[i] = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(we,b,sample):\n",
    "#     sample.pop(0)\n",
    "    prediction = -1 if np.dot(np.transpose(we), sample) + b <= 0 else 1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataset(weights,bias,data):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        row = []\n",
    "        row.append(data[i,0])\n",
    "        for key,value in weights.items():\n",
    "            x = data[i,1:]\n",
    "#             del prediction_rows[:]\n",
    "            p=prediction(value,bias[key],x)\n",
    "#             p = prediction_rows[0]\n",
    "            row.append(p)\n",
    "        l.append(row)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_dataset_eval(weights,bias,data):\n",
    "    l = []\n",
    "    for i in range(len(data)):\n",
    "        row = []\n",
    "#         row.append(data[i,0])\n",
    "        for key,value in weights.items():\n",
    "            x = data[i,:]\n",
    "#             del prediction_rows[:]\n",
    "            p=prediction(value,bias[key],x)\n",
    "#             p = prediction_rows[0]\n",
    "            row.append(p)\n",
    "        l.append(row)\n",
    "    return np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for testing ######\n",
    "defendants_age = []\n",
    "defendant_gender=[]\n",
    "num_victims=[]\n",
    "offence_category=[]\n",
    "offence_subcategory=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_age(test)\n",
    "factorize_gender(test)\n",
    "factorize_offence_category(test)\n",
    "factorize_offence_subcategory(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_victims = list(test['num_victims'])\n",
    "label= list(df_glove_test.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(list(zip(label,defendants_age,defendant_gender,num_victims,offence_category,offence_subcategory)),  columns =[\"label\",\"defendants_age\" ,\"defendants_gender\",\"num_victims\",\"offence_category\",\"offence_sub_category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numpy = df_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### for eval #######\n",
    "defendants_age = []\n",
    "defendant_gender=[]\n",
    "num_victims=[]\n",
    "offence_category=[]\n",
    "offence_subcategory=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "factorize_gender(eval)\n",
    "factorize_offence_category(eval)\n",
    "factorize_offence_subcategory(eval)\n",
    "num_victims = list(eval['num_victims'])\n",
    "label = [0 for i in range(len(eval))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,age in enumerate(eval['defendant_age']):\n",
    "#         print(\"age\",age)\n",
    "        # print(i)\n",
    "        age = age.strip(\"(  ) -\")\n",
    "        if age != \"not known\":\n",
    "            age = age.replace(\"years\",\"\")\n",
    "            age = age.replace(\"about\", \"\")\n",
    "            age = age.replace(\"age\",\"\")\n",
    "            age = age.replace(\"of\", \"\" )\n",
    "            age = age.replace(\"old\", \"\")\n",
    "            age = age.replace(\"Year\", \"\")\n",
    "            age = age.replace(\"his\", \"\")\n",
    "            age = age.replace(\"Age\", \"\")\n",
    "            age = age.strip()\n",
    "            age = age.strip(\"d\")\n",
    "            \n",
    "            if age.find(\" \") >= 0:\n",
    "                temp = age.split(\" \")\n",
    "                if \"and\" in temp or \"or\" in temp:\n",
    "                    age = temp[0]\n",
    "#                     temp.remove(\"and\")\n",
    "#                 if \"months\" in temp or \"month\" in temp:\n",
    "#                     temp.remove(\"months\")\n",
    "# #                     temp.remove(\"month\")\n",
    "                # print(temp)\n",
    "                else:\n",
    "                    age = '-'.join(temp)\n",
    "#             print(\"**\",age)\n",
    "            syns = wordnet.synsets(age.strip())\n",
    "#             print(\"syns\",syns)\n",
    "            # print(\"A\",age)\n",
    "            # print(\"S\",syns[0].lemmas()[0].name())\n",
    "            age = syns[0].lemmas()[0].name()\n",
    "            if age.find(\"-\") >= 0:\n",
    "                temp = age.split(\"-\")\n",
    "                # print(temp)\n",
    "                age = ' '.join(temp)\n",
    "#             print(age)\n",
    "#             defendants_age.append(w2n.word_to_num(age.strip()))\n",
    "            try:\n",
    "                defendants_age.append(w2n.word_to_num(age.strip()))\n",
    "            except:\n",
    "                defendants_age.append(int(0))\n",
    "        else:\n",
    "            defendants_age.append(int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(list(zip(defendants_age,defendant_gender,num_victims,offence_category,offence_subcategory)),  columns =[\"defendants_age\" ,\"defendants_gender\",\"num_victims\",\"offence_category\",\"offence_sub_category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_numpy = df_eval.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tree = new_dataset(weights,bias,train_numpy)\n",
    "test_tree = new_dataset(weights,bias,test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_tree = new_dataset_eval(weights,bias,eval_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### now apply ID3 on this as we dont have to bin it any more #####\n",
    "class treenode:\n",
    "    def __init__(self, featureName):\n",
    "        #print(\"featurename\",featureName)\n",
    "        self.featureName = featureName\n",
    "        self.children = []\n",
    "\n",
    "def findmajoritylabel(dataset):\n",
    "    n = list(dataset[:,0]).count(-1)\n",
    "    p = list(dataset[:,0]).count(1)\n",
    "    if n > p:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def check_all_label_same(dataset):\n",
    "    if len(dataset) == list(dataset[:,0]).count(1):\n",
    "        return 1\n",
    "    if len(dataset) == list(dataset[:,0]).count(-1):\n",
    "        return -1\n",
    "\n",
    "def cal_entropy(dataset,attributeval=None, feature=None):\n",
    "    nrows = len(dataset)\n",
    "    if feature == None:\n",
    "        p = list(dataset[:,0]).count(1)\n",
    "        n = list(dataset[:,0]).count(-1)\n",
    "        if p / nrows == 0 or n / nrows == 0:\n",
    "            # Entropy_S = 0\n",
    "            return 0\n",
    "        entropy = - ((p / nrows) * math.log((p / nrows), 2)) - ((n / nrows) * math.log((n / nrows), 2))\n",
    "        return  entropy\n",
    "    else:\n",
    "        len_dataset_feature = dataset[dataset[:,feature] == attributeval]\n",
    "        if len(len_dataset_feature) == 0:\n",
    "            return  0\n",
    "        else:\n",
    "            p = list(len_dataset_feature[:,0]).count(1)/len(len_dataset_feature)\n",
    "            n = list(len_dataset_feature[:,0]).count(-1)/len(len_dataset_feature)\n",
    "            if p == 0 or n == 0:\n",
    "                return 0\n",
    "            entropy = -  p* math.log(p,2)  - n*math.log(n,2)\n",
    "            # print(feature,attributeval,(len(len_dataset_feature)/nrows)*entropy)\n",
    "            return (len(len_dataset_feature)/nrows)*entropy\n",
    "\n",
    "def cal_max_gain(dataset, feature_set):\n",
    "    dataset_entropy =  cal_entropy(dataset)\n",
    "    # print(\"entropy\",dataset_entropy)\n",
    "    max_gain = -1\n",
    "    selected_feature = ''\n",
    "    for f in feature_set:\n",
    "        entropy_feature = 0\n",
    "        for all_vals in set(dataset[:,f]):\n",
    "            entropy_feature = entropy_feature + cal_entropy(dataset,all_vals,f)\n",
    "        gain = dataset_entropy - entropy_feature\n",
    "        # print(\"gain\",gain)\n",
    "        # print(f,gain)\n",
    "        if max_gain < gain:\n",
    "            max_gain = gain\n",
    "            selected_feature = f\n",
    "    # print(selected_feature)\n",
    "    return selected_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(dataset,featureset,current_depth,depth):\n",
    "    # print(\"length\",dataset)\n",
    "    if current_depth + 1 > depth:\n",
    "        return treenode(findmajoritylabel(dataset))\n",
    "    #check if all labels are the same, if yes return a node with the label\n",
    "    if check_all_label_same(dataset) == -1:  #guilty\n",
    "        #print(\"label 0\" )\n",
    "        return treenode(-1)\n",
    "    if check_all_label_same(dataset) == 1:  #not guilty\n",
    "        #print(\"label 1\")\n",
    "        return treenode(1)\n",
    "    if len(featureset) == 0:\n",
    "        # print(\"empty\")\n",
    "        return treenode(findmajoritylabel(dataset))\n",
    "    #find feature with max info gain\n",
    "    new_selected_feature = cal_max_gain(dataset, featureset)\n",
    "    # print(\"n\",new_selected_feature)\n",
    "    node = treenode(new_selected_feature)\n",
    "    children = []\n",
    "    set_of_possible_values = set(dataset[:,new_selected_feature])\n",
    "    for v in set_of_possible_values:\n",
    "        sv =  dataset[dataset[:,new_selected_feature] == v]\n",
    "        if len(sv) == 0:\n",
    "            # print(\"entered empty\",new_selected_feature,v)\n",
    "            return treenode(findmajoritylabel(dataset))\n",
    "        else:\n",
    "            new_feature_set = featureset.copy()\n",
    "            new_feature_set.remove(new_selected_feature)\n",
    "            # childnode = ID3(sv, new_feature_set)\n",
    "            childnode = ID3(sv, new_feature_set,current_depth+1,depth)\n",
    "            child_dict = {\"value\": v, \"child\": childnode}\n",
    "            children.append(child_dict)\n",
    "    node.children = children\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_t(row, root):\n",
    "    if not root.children:\n",
    "        # print(root.featureName)\n",
    "        prediction_rows.append(root.featureName)\n",
    "        return root.featureName\n",
    "\n",
    "    decision_to_take = row[root.featureName]\n",
    "    # print(root.featureName)\n",
    "    # print(decision_to_take)\n",
    "    index = 0\n",
    "    for i in range(len(root.children)):\n",
    "        if root.children[i]['value'] == decision_to_take:\n",
    "            index = i\n",
    "    prediction_t(row, root.children[index]['child'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracyCal(data, root):\n",
    "    list_p = []\n",
    "    for i in range(0, len(data)):\n",
    "        # print(i)\n",
    "        v = prediction_t(list(data[i,:]), root)\n",
    "        # prediction_rows.append(v)\n",
    "        # print(v)\n",
    "    correct_predictions = 0\n",
    "    wrong_predictiosn = 0\n",
    "    # print(len(prediction_rows))\n",
    "    # print(len(data))\n",
    "    # for i in range(0, len(data)):\n",
    "    #     if data.iloc[i][0] == prediction_rows[i]:\n",
    "    #         correct_predictions += 1\n",
    "    # print((correct_predictions) / len(data))\n",
    "    # return prediction_rows\n",
    "    for i in range(0, len(data)):\n",
    "        if data[i,0] == prediction_rows[i]:\n",
    "            correct_predictions += 1\n",
    "    return (correct_predictions) / len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = [i for i in range(1,201)]\n",
    "feature_set_eval = [i for i in range(1,200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "root= ID3(train_tree,feature_set,0,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracyCal(train_tree,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820571428571429"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracyCal(test_tree,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7897777777777778"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_tree_depth(node,depth):\n",
    "    if len(node.children) == 0:\n",
    "        return depth\n",
    "    max_depth = 0\n",
    "    for child_node in node.children:\n",
    "        child_node_depth = cal_tree_depth(child_node['child'], depth + 1)\n",
    "        if max_depth < child_node_depth:\n",
    "            max_depth = child_node_depth\n",
    "    return max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_tree_depth(root,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_t_e(row, root):\n",
    "    if not root.children:\n",
    "        # print(root.featureName)\n",
    "        prediction_rows.append(root.featureName)\n",
    "        return root.featureName\n",
    "\n",
    "    decision_to_take = row[root.featureName]\n",
    "    # print(root.featureName)\n",
    "    # print(decision_to_take)\n",
    "    index = 0\n",
    "    for i in range(len(root.children)):\n",
    "        if root.children[i]['value'] == decision_to_take:\n",
    "            index = i\n",
    "    prediction_t_e(row, root.children[index]['child'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracyCal_e(data, root):\n",
    "    list_p = []\n",
    "    for i in range(0, len(data)):\n",
    "        # print(i)\n",
    "        v = prediction_t(list(data[i,:]), root)\n",
    "        # prediction_rows.append(v)\n",
    "        # print(v)\n",
    "    correct_predictions = 0\n",
    "    wrong_predictiosn = 0\n",
    "    # print(len(prediction_rows))\n",
    "    # print(len(data))\n",
    "    # for i in range(0, len(data)):\n",
    "    #     if data.iloc[i][0] == prediction_rows[i]:\n",
    "    #         correct_predictions += 1\n",
    "    # print((correct_predictions) / len(data))\n",
    "    return prediction_rows\n",
    "#     for i in range(0, len(data)):\n",
    "#         if data[i,0] == prediction_rows[i]:\n",
    "#             correct_predictions += 1\n",
    "#     return (correct_predictions) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = accuracyCal_e(eval_tree,root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('perceptron_id3_ensemble.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"example_id\", \"label\"])\n",
    "    for i in range(len(p)):\n",
    "        if p[i]== -1:\n",
    "            val = 0\n",
    "        else:\n",
    "            val = 1\n",
    "        writer.writerow([i, val])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
